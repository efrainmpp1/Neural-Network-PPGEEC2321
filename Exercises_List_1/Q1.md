# Questão 1

As métricas de distancia entre vetores são aplicadas nos estudos de aprendizagem de
máquina como medidas de similaridade/dissimilaridade entre vetores que representam padrões
(atributos).

Apresente um estudo sobre as seguintes distancias:

- Distancia Euclidiana,
- Distancia City Block
- Distancia de Minkowski
- Distancia de Mahalanobis
- Coeficiente de Correlação de Pearson
- Similaridade Cosseno.

Apresente neste estudo aplicações onde cada tipo de métricas de distância é mais adequada.

Sugestão de Aplicações:

- Classificação de padrões
- Clustering
- Reconhecimento de padrões
- Modelos de Linguagem Natural
- ...

# Sessão Resposta

Nesta sessão, será apresentado um estudo detalhado sobre as métricas de distância mencionadas anteriormente.
Serão discutidas suas definições, propriedades e aplicações práticas em diferentes contextos, como
classificação de padrões, clustering, reconhecimento de padrões e modelos de linguagem natural. O objetivo é
compreender em quais situações cada métrica é mais adequada e como elas podem ser utilizadas para resolver
problemas específicos em aprendizado de máquina.

## Distancia Euclidiana

A distância Euclidiana representa uma das medidas mais frequentes em aprendizado de máquina para quantificar a semelhança entre dois vetores. Sua definição envolve a raiz quadrada da adição dos quadrados das diferenças
verificadas em cada par de componentes equivalentes nos vetores.

A fórmula da distância Euclidiana entre dois vetores $\mathbf{u} = (u_1, u_2, \ldots, u_n)$ e $\mathbf{v} = (v_1, v_2, \ldots, v_n)$

Em um espaço $\mathbf{n}$-dimensional é dada por:

$d(\mathbf{u}, \mathbf{v}) = \sqrt{(u_1 - v_1)^2 + (u_2 - v_2)^2 + \ldots + (u_n - v_n)^2}$

Ou seja, podemos descrever como

```math
d(\mathbf{u}, \mathbf{v}) = \sqrt{\sum_{i=1}^n (u_i - v_i)^2}
```

Essa métrica é amplamente utilizada devido à sua simplicidade e eficácia em medir a similaridade em espaços vetoriais. Ela também satisfaz propriedades matemáticas como a **não-negatividade, identidade dos indiscerníveis, simetria e desigualdade triangular**. A posse dessas propriedades não é trivial e confere robustez e interpretabilidade às métricas utilizadas.

**A relevância de cada propriedade:**

- **Não-negatividade:** Esta propriedade garante que a distância entre quaisquer dois pontos seja sempre maior ou igual a zero ( $d(\mathbf{u}, \mathbf{v}) >= 0$ ). Uma distância negativa não faria sentido no espaço métrico. Essa característica assegura que a medida de dissimilaridade seja sempre um valor físico ou conceitualmente consistente.

- **Identidade dos indiscerníveis:** Esta propriedade estabelece que a distância entre um ponto e ele mesmo é sempre zero, e que se a distância entre dois pontos é zero, então esses dois pontos são o mesmo ( $d(\mathbf{u}, \mathbf{u}) = 0$ ). Isso é essencial para garantir que a métrica distinga corretamente entre diferentes entidades e que a similaridade máxima ocorra apenas entre objetos idênticos.

- **Simetria:** A simetria implica que a distância entre o ponto A e o ponto B é a mesma que a distância entre o ponto B e o ponto A ( $d(\mathbf{u}, \mathbf{v})  = d(\mathbf{v}, \mathbf{u})$ ). Em muitos cenários de análise de dados, a relação de "distância" ou "dissimilaridade" é inerentemente bidirecional, tornando essa propriedade fundamental para uma representação consistente das relações entre os dados.

- **Desigualdade Triangular:** Esta propriedade afirma que, para quaisquer três pontos A, B e C, a distância entre A e C é sempre menor ou igual à soma das distâncias entre A e B e entre B e C. Intuitivamente, o caminho mais curto entre dois pontos é uma linha reta ( $d(\mathbf{u}, \mathbf{v})  <=  d(\mathbf{u}, \mathbf{k}) + d(\mathbf{k}, \mathbf{v})$ ). Essa propriedade é crucial para garantir a consistência geométrica do espaço métrico e evita situações onde "atalhos" seriam maiores que os caminhos indiretos, o que comprometeria a interpretação das relações de distância.

Em suma, a satisfação dessas quatro propriedades matemáticas confere às métricas de distância uma base teórica sólida, permitindo que os algoritmos de aprendizado de máquina operem de maneira previsível e interpretem as relações entre os dados de forma coerente com as noções intuitivas de "distância" e "similaridade". A ausência de uma ou mais dessas propriedades pode levar a resultados inesperados, interpretações errôneas e, em última análise, à ineficácia dos modelos construídos. Portanto, reconhecer e valorizar métricas que atendem a esses critérios é fundamental para a construção de soluções robustas e confiáveis em ciência de dados.

## Distância de Manhattan (City Block)

A distância de Manhattan, também conhecida como distância City Block ou distância L1, é uma métrica que calcula a soma das diferenças absolutas entre as coordenadas correspondentes de dois vetores. Diferentemente da distância Euclidiana, que considera o caminho mais curto em linha reta, a distância de Manhattan mede a distância percorrida ao longo de eixos ortogonais, como se estivéssemos caminhando em um grid de ruas.

A fórmula da distância de Manhattan entre dois vetores $\mathbf{u} = (u_1, u_2, \ldots, u_n)$ e $\mathbf{v} = (v_1, v_2, \ldots, v_n)$ é dada por:

$d(\mathbf{u}, \mathbf{v}) = |(u_1 - u_1)| + |(u_2 - v_2)| + \ldots + |(u_n - v_n)|$

Ou seja podemos descrever essa equação como:

```math
d(\mathbf{u}, \mathbf{v}) = \sum_{i=1}^n |u_i - v_i|
```

Assim como a distância Euclidiana, a distância de Manhattan também satisfaz as propriedades de não-negatividade, identidade dos indiscerníveis, simetria e desigualdade triangular. No entanto, ela é mais adequada para situações em que o movimento ocorre em trajetórias ortogonais, como em redes urbanas ou sistemas de grade.

A simplicidade da distância de Manhattan e sua adequação a contextos específicos a tornam uma métrica valiosa em diversas aplicações práticas. Ela é especialmente útil quando o objetivo é capturar diferenças absolutas entre vetores em vez de diferenças quadráticas.

## Métrica de Minkowski

A métrica de Minkowski é uma generalização das distâncias Euclidiana e Manhattan, permitindo ajustar o cálculo da distância entre dois vetores por meio de um parâmetro $p$. Dependendo do valor de $p$, a métrica de Minkowski pode se comportar como a distância Euclidiana ($p = 2$) ou como a distância de Manhattan ($p = 1$).

A fórmula da métrica de Minkowski entre dois vetores $\mathbf{u} = (u_1, u_2, \ldots, u_n)$ e $\mathbf{v} = (v_1, v_2, \ldots, v_n)$ é dada por:

```math
d(\mathbf{u}, \mathbf{v}) = \left( \sum_{i=1}^n |u_i - v_i|^p \right)^{\frac{1}{p}}
```

Assim como as distâncias Euclidiana e Manhattan, a métrica de Minkowski satisfaz as propriedades de não-negatividade, identidade dos indiscerníveis, simetria e desigualdade triangular. O parâmetro $p$ permite ajustar a sensibilidade da métrica às diferenças entre as coordenadas dos vetores.

### Casos Especiais

- **Distância de Manhattan ($p = 1$):**

  $d(\mathbf{u}, \mathbf{v}) = \sum_{i=1}^n |u_i - v_i|$

  Neste caso, a métrica mede a soma das diferenças absolutas entre as coordenadas.

- **Distância Euclidiana ($p = 2$):**

  $d(\mathbf{u}, \mathbf{v}) = \sqrt{\sum_{i=1}^n (u_i - v_i)^2}$

  Aqui, a métrica mede a distância em linha reta entre os dois vetores.

- **Limite para $p \to \infty$ (Distância de Chebyshev):**

  $d(\mathbf{u}, \mathbf{v}) = \max_{i=1}^n |u_i - v_i|$

  Neste caso, a métrica considera a maior diferença absoluta entre as coordenadas.

### Aplicações

A métrica de Minkowski é amplamente utilizada em aprendizado de máquina e análise de dados, especialmente em algoritmos de clustering e classificação, como o k-Nearest Neighbors (k-NN). A escolha do parâmetro $p$ depende do problema em questão e da natureza dos dados. Por exemplo:

- Valores menores de $p$ dão mais peso às pequenas diferenças entre as coordenadas.
- Valores maiores de $p$ enfatizam as maiores diferenças.

Essa flexibilidade torna a métrica de Minkowski uma ferramenta poderosa para medir similaridade em diferentes contextos e domínios.

## Distancia de Mahalanobis

A distância de Mahalanobis é uma métrica que mede a distância entre dois pontos em um espaço multivariado, levando em consideração a correlação entre as variáveis. Diferentemente de métricas como a Euclidiana, a distância de Mahalanobis ajusta a escala das variáveis e considera a distribuição dos dados, tornando-a particularmente útil em cenários onde as variáveis possuem diferentes variâncias ou estão correlacionadas.

A fórmula da distância de Mahalanobis entre dois vetores $\mathbf{u}$ e $\mathbf{v}$, com base na matriz de covariância $\mathbf{S}$ dos dados, é dada por:

```math
d(\mathbf{u}, \mathbf{v}) = \sqrt{(\mathbf{u} - \mathbf{v})^\top \mathbf{S}^{-1} (\mathbf{u} - \mathbf{v})}
```

### Propriedades

1. **Consideração da Covariância:** A matriz de covariância $\mathbf{S}$ captura as relações entre as variáveis, permitindo que a métrica leve em conta a estrutura dos dados.
2. **Escala Invariante:** A distância de Mahalanobis é invariante a mudanças de escala nas variáveis, o que a torna adequada para dados com diferentes unidades ou magnitudes.
3. **Detecção de Outliers:** Devido à sua sensibilidade à distribuição dos dados, essa métrica é frequentemente usada para identificar outliers em conjuntos de dados multivariados.

A distância de Mahalanobis é especialmente poderosa em contextos onde a estrutura interna dos dados desempenha um papel importante, permitindo análises mais robustas e precisas em comparação com métricas que não consideram a correlação entre variáveis.

### Exemplo de Aplicação

Um trabalho que ilustra a eficácia da distância de Mahalanobis em detectar **falhas em processos industriais** pode ser encontrado no estudo de DAMIANI (2021) "Uso da Distância de Mahalanobis para a Detecção de Rom pimentos em Linhas de
Injeção de Gás". Nesse trabalho, a distância de Mahalanobis é usada para capturar a estrutura de correlação em dados de alta dimensão, o que é particularmente útil em ambientes industriais onde muitas variáveis estão envolvidas.

Essa abordagem permite uma detecção mais precisa e rápida de falhas em comparação com métodos baseados na distância euclidiana, pois considera as correlações entre as variáveis do processo.

## Coeficiente de Correlação de Pearson

O coeficiente de correlação de Pearson é uma medida estatística que avalia a força e a direção da relação linear entre duas variáveis contínuas.

A fórmula para calcular o coeficiente de correlação de Pearson entre duas variáveis $X$ e $Y$ é dada por:

$r = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum_{i=1}^n (X_i - \bar{X})^2 \sum_{i=1}^n (Y_i - \bar{Y})^2}}$

Onde:

- $X_i$ e $Y_i$ são os valores individuais das variáveis $X$ e $Y$,
- $\bar{X}$ e $\bar{Y}$ são as médias das variáveis $X$ e $Y$,
- $n$ é o número de observações.

O valor de $r$ varia entre -1 e 1:

- **$r = 1$** indica uma correlação linear positiva perfeita,
- **$r = -1$** indica uma correlação linear negativa perfeita,
- **$r = 0$** indica ausência de correlação linear.

### Interpretação

MUKAKA (2012) destaca que, embora o coeficiente de correlação de Pearson seja útil para identificar relações lineares, ele não captura associações não lineares. Além disso, sua interpretação deve ser feita com cautela, pois valores extremos ou outliers podem influenciar significativamente o resultado.

### Aplicações

O coeficiente de correlação de Pearson é amplamente utilizado em aprendizado de máquina para avaliar a relação entre variáveis de entrada e saída, em análise de dados para identificar padrões e em estudos científicos para validar hipóteses sobre associações entre variáveis.

# Referências

- DAMIANI, E. J. Uso da Distância de Mahalanobis para a Detecção de Rompimentos em Linhas de Injeção de Gás, 2021. Disponível em: [https://lume.ufrgs.br/bitstream/handle/10183/235773/001136697.pdf?sequence=1&isAllowed=y](https://lume.ufrgs.br/bitstream/handle/10183/235773/001136697.pdf?sequence=1&isAllowed=y). Acesso em: 15 de abr. 2025.

- MUKAKA, M. M. A guide to appropriate use of correlation coefficient in medical research. _Malawi Medical Journal_, v. 24, n. 3, p. 69–71, 2012.
