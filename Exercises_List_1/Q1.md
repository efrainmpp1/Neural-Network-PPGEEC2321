# Questão 1

As métricas de distancia entre vetores são aplicadas nos estudos de aprendizagem de
máquina como medidas de similaridade/dissimilaridade entre vetores que representam padrões
(atributos).

Apresente um estudo sobre as seguintes distancias:

- Distancia Euclidiana,
- Distancia de Minkowski
- Distancia City Block
- Distancia de Mahalanobis
- Coeficiente de Correlação de Pearson
- Similaridade Cosseno.

Apresente neste estudo aplicações onde cada tipo de métricas de distância é mais adequada.

Sugestão de Aplicações:

- Classificação de padrões
- Clustering
- Reconhecimento de padrões
- Modelos de Linguagem Natural
- ...

# Sessão Resposta

Nesta sessão, será apresentado um estudo detalhado sobre as métricas de distância mencionadas anteriormente.
Serão discutidas suas definições, propriedades e aplicações práticas em diferentes contextos, como
classificação de padrões, clustering, reconhecimento de padrões e modelos de linguagem natural. O objetivo é
compreender em quais situações cada métrica é mais adequada e como elas podem ser utilizadas para resolver
problemas específicos em aprendizado de máquina.

## Distancia Euclidiana

A distância Euclidiana representa uma das medidas mais frequentes em aprendizado de máquina para quantificar a semelhança entre dois vetores. Sua definição envolve a raiz quadrada da adição dos quadrados das diferenças
verificadas em cada par de componentes equivalentes nos vetores.

A fórmula da distância Euclidiana entre dois vetores $\mathbf{u} = (u_1, u_2, \ldots, u_n)$ e $\mathbf{v} = (v_1, v_2, \ldots, v_n)$

Em um espaço $\mathbf{n}$-dimensional é dada por:

$d(\mathbf{u}, \mathbf{v}) = \sqrt{(u_1 - v_1)^2 + (u_2 - v_2)^2 + \ldots + (u_n - v_n)^2}$

Ou seja, podemos descrever como

```math
d(\mathbf{u}, \mathbf{v}) = \sqrt{\sum_{i=1}^n (u_i - v_i)^2}
```

Essa métrica é amplamente utilizada devido à sua simplicidade e eficácia em medir a similaridade em espaços vetoriais. Ela também satisfaz propriedades matemáticas como a **não-negatividade, identidade dos indiscerníveis, simetria e desigualdade triangular**. A posse dessas propriedades não é trivial e confere robustez e interpretabilidade às métricas utilizadas.

**A relevância de cada propriedade:**

- **Não-negatividade:** Esta propriedade garante que a distância entre quaisquer dois pontos seja sempre maior ou igual a zero ( $d(\mathbf{u}, \mathbf{v}) >= 0$ ). Uma distância negativa não faria sentido no espaço métrico. Essa característica assegura que a medida de dissimilaridade seja sempre um valor físico ou conceitualmente consistente.

- **Identidade dos indiscerníveis:** Esta propriedade estabelece que a distância entre um ponto e ele mesmo é sempre zero, e que se a distância entre dois pontos é zero, então esses dois pontos são o mesmo ( $d(\mathbf{u}, \mathbf{u}) = 0$ ). Isso é essencial para garantir que a métrica distinga corretamente entre diferentes entidades e que a similaridade máxima ocorra apenas entre objetos idênticos.

- **Simetria:** A simetria implica que a distância entre o ponto A e o ponto B é a mesma que a distância entre o ponto B e o ponto A ( $d(\mathbf{u}, \mathbf{v})  = d(\mathbf{v}, \mathbf{u})$ ). Em muitos cenários de análise de dados, a relação de "distância" ou "dissimilaridade" é inerentemente bidirecional, tornando essa propriedade fundamental para uma representação consistente das relações entre os dados.

- **Desigualdade Triangular:** Esta propriedade afirma que, para quaisquer três pontos A, B e C, a distância entre A e C é sempre menor ou igual à soma das distâncias entre A e B e entre B e C. Intuitivamente, o caminho mais curto entre dois pontos é uma linha reta ( $d(\mathbf{u}, \mathbf{v})  <=  d(\mathbf{u}, \mathbf{k}) + d(\mathbf{k}, \mathbf{v})$ ). Essa propriedade é crucial para garantir a consistência geométrica do espaço métrico e evita situações onde "atalhos" seriam maiores que os caminhos indiretos, o que comprometeria a interpretação das relações de distância.

Em suma, a satisfação dessas quatro propriedades matemáticas confere às métricas de distância uma base teórica sólida, permitindo que os algoritmos de aprendizado de máquina operem de maneira previsível e interpretem as relações entre os dados de forma coerente com as noções intuitivas de "distância" e "similaridade". A ausência de uma ou mais dessas propriedades pode levar a resultados inesperados, interpretações errôneas e, em última análise, à ineficácia dos modelos construídos. Portanto, reconhecer e valorizar métricas que atendem a esses critérios é fundamental para a construção de soluções robustas e confiáveis em ciência de dados.

## Distância de Manhattan

A distância de Manhattan, também conhecida como distância City Block ou distância L1, é uma métrica que calcula a soma das diferenças absolutas entre as coordenadas correspondentes de dois vetores. Diferentemente da distância Euclidiana, que considera o caminho mais curto em linha reta, a distância de Manhattan mede a distância percorrida ao longo de eixos ortogonais, como se estivéssemos caminhando em um grid de ruas.

A fórmula da distância de Manhattan entre dois vetores $\mathbf{u} = (u_1, u_2, \ldots, u_n)$ e $\mathbf{v} = (v_1, v_2, \ldots, v_n)$ é dada por:

$d(\mathbf{u}, \mathbf{v}) = |(u_1 - u_1)| + |(u_2 - v_2)| + \ldots + |(u_n - v_n)|$

Ou seja podemos descrever essa equação como:

```math
d(\mathbf{u}, \mathbf{v}) = \sum_{i=1}^n |u_i - v_i|
```

Assim como a distância Euclidiana, a distância de Manhattan também satisfaz as propriedades de não-negatividade, identidade dos indiscerníveis, simetria e desigualdade triangular. No entanto, ela é mais adequada para situações em que o movimento ocorre em trajetórias ortogonais, como em redes urbanas ou sistemas de grade.

A simplicidade da distância de Manhattan e sua adequação a contextos específicos a tornam uma métrica valiosa em diversas aplicações práticas. Ela é especialmente útil quando o objetivo é capturar diferenças absolutas entre vetores em vez de diferenças quadráticas.

## Métrica de Minkowski

A métrica de Minkowski é uma generalização das distâncias Euclidiana e Manhattan, permitindo ajustar o cálculo da distância entre dois vetores por meio de um parâmetro $p$. Dependendo do valor de $p$, a métrica de Minkowski pode se comportar como a distância Euclidiana ($p = 2$) ou como a distância de Manhattan ($p = 1$).

A fórmula da métrica de Minkowski entre dois vetores $\mathbf{u} = (u_1, u_2, \ldots, u_n)$ e $\mathbf{v} = (v_1, v_2, \ldots, v_n)$ é dada por:

```math
d(\mathbf{u}, \mathbf{v}) = \left( \sum_{i=1}^n |u_i - v_i|^p \right)^{\frac{1}{p}}
```

Assim como as distâncias Euclidiana e Manhattan, a métrica de Minkowski satisfaz as propriedades de não-negatividade, identidade dos indiscerníveis, simetria e desigualdade triangular. O parâmetro $p$ permite ajustar a sensibilidade da métrica às diferenças entre as coordenadas dos vetores.

### Casos Especiais

- **Distância de Manhattan ($p = 1$):**

  $d(\mathbf{u}, \mathbf{v}) = \sum_{i=1}^n |u_i - v_i|$

  Neste caso, a métrica mede a soma das diferenças absolutas entre as coordenadas.

- **Distância Euclidiana ($p = 2$):**

  $d(\mathbf{u}, \mathbf{v}) = \sqrt{\sum_{i=1}^n (u_i - v_i)^2}$

  Aqui, a métrica mede a distância em linha reta entre os dois vetores.

- **Limite para $p \to \infty$ (Distância de Chebyshev):**

  $d(\mathbf{u}, \mathbf{v}) = \max_{i=1}^n |u_i - v_i|$

  Neste caso, a métrica considera a maior diferença absoluta entre as coordenadas.

### Aplicações

A métrica de Minkowski é amplamente utilizada em aprendizado de máquina e análise de dados, especialmente em algoritmos de clustering e classificação, como o k-Nearest Neighbors (k-NN). A escolha do parâmetro $p$ depende do problema em questão e da natureza dos dados. Por exemplo:

- Valores menores de $p$ dão mais peso às pequenas diferenças entre as coordenadas.
- Valores maiores de $p$ enfatizam as maiores diferenças.

Essa flexibilidade torna a métrica de Minkowski uma ferramenta poderosa para medir similaridade em diferentes contextos e domínios.
