# Questão 3

Apresente um estudo comparando os seguintes algoritmos de otimização:

- Gradiente Estocástico (SGM)
- AdaGrad
- RMSProp
- Adam

Estes métodos ou otimizadores são utilizados no processo de aprendizagem de redes neurais/deep learning.

Os algoritmos de otimização desempenham um papel crucial no
treinamento de redes neurais, ajustando seus parâmetros para minimizar a função de perda e, consequentemente, melhorar o desempenho do modelo. Um exemplo clássico é o Gradient Descent (GD), que utiliza o gradiente da função de custo para atualizar os pesos da rede em direção ao mínimo global. No entanto, o GD pode enfrentar dificuldades, como ficar preso em mínimos locais. Para superar essas limitações, surgiram variações e métodos mais avançados, como o SGD, o AdaGrad, o RMSProp e o Adam, que serão discutidos em detalhes a seguir.

## Gradiente Estocástico (SGM)

O Gradiente Estocástico (SGM) é uma variação do método de Gradiente Descendente, amplamente utilizado para otimização em aprendizado de máquina. No método do Gradiente Descendente (GD), a atualização dos pesos é realizada utilizando o gradiente calculado em relação a todo o conjunto de dados.

A fórmula de atualização é dada por:

$\theta(t+1) = \theta(t) - \eta \nabla L(\theta(t))$

Onde:

- $\theta(t)$ representa os parâmetros do modelo na iteração $t$.
- $\eta$ é a taxa de aprendizado.
- $\nabla L(\theta(t))$ é o gradiente da função de perda $L$ em relação aos parâmetros $\theta(t)$, calculado para todo o conjunto de dados.

Embora o GD seja eficaz para encontrar mínimos globais em funções convexas, ele pode ser computacionalmente caro em grandes conjuntos de dados, pois requer o cálculo do gradiente para todos os exemplos a cada iteração.

A melhoria introduzida pelo **Gradiente Estocástico (SGM)** está na redução do custo computacional por iteração. Em vez de calcular o gradiente para todo o conjunto de dados, o SGM utiliza apenas um exemplo ou um mini-batch, como mostrado anteriormente. Isso permite atualizações mais frequentes dos pesos, acelerando o processo de treinamento e tornando-o mais eficiente para grandes volumes de dados. Além disso, o ruído introduzido pelo SGM pode ajudar o modelo a escapar de mínimos locais, o que é uma vantagem em problemas não convexos.

A atualização dos pesos no SGM segue a fórmula:

$\theta(t+1) = \theta(t) - \eta \nabla L(\theta(t); x_i, y_i)$

Onde:

- $\theta(t)$ representa os parâmetros do modelo na iteração $t$.
- $\eta$ é a taxa de aprendizado.
- $\nabla L(\theta(t); x_i, y_i)$ é o gradiente da função de perda $L$ em relação aos parâmetros $\theta(t)$, calculado para o exemplo $(x_i, y_i)$.
- $(x_i, y_i)$ é um par de exemplo de entrada e saída do conjunto de dados.

Essa abordagem reduz o custo computacional por iteração, tornando o treinamento mais rápido, especialmente em grandes conjuntos de dados. No entanto, o SGM introduz ruído no cálculo do gradiente, o que pode levar a oscilações durante a convergência. Apesar disso, o ruído pode ajudar o modelo a escapar de mínimos locais, tornando-o útil em problemas complexos.

O Gradiente Estocástico é simples de implementar e serve como base para muitos algoritmos de otimização mais avançados, como **AdaGrad**, **RMSProp** e **Adam**.

## AdaGrad

O **AdaGrad** (Adaptive Gradient Algorithm) é um método de otimização que busca melhorar o desempenho do Gradiente Estocástico (SGM) ao adaptar a taxa de aprendizado para cada parâmetro do modelo de forma individual. A principal proposta do AdaGrad é lidar com o problema de escalas diferentes nos gradientes dos parâmetros, ajustando automaticamente a magnitude das atualizações com base no histórico de gradientes acumulados.

A ideia central do AdaGrad é acumular os quadrados dos gradientes de cada parâmetro ao longo do tempo e usar essa informação para ajustar a taxa de aprendizado. Parâmetros que possuem gradientes maiores recebem atualizações menores, enquanto parâmetros com gradientes menores recebem atualizações maiores. Isso permite que o algoritmo se adapte dinamicamente às características do problema, promovendo uma convergência mais eficiente.

Esse processo permite que o AdaGrad ajuste dinamicamente a magnitude das atualizações para cada parâmetro, promovendo uma convergência mais eficiente em problemas com características variadas.

A fórmula de atualização do AdaGrad é dada por:

$\theta(t+1) = \theta(t) - \frac{\eta}{\sqrt{G_{t,ii} + \epsilon}} \nabla L(\theta(t))$

Onde:

- $\theta(t)$ representa os parâmetros do modelo na iteração $t$.
- $\eta$ é a taxa de aprendizado inicial.
- $G_{t,ii}$ é a soma acumulada dos quadrados dos gradientes para o parâmetro $i$ até a iteração $t$.
- $\epsilon$ é um pequeno valor adicionado para evitar divisões por zero.
- $\nabla L(\theta(t))$ é o gradiente da função de perda em relação aos parâmetros $\theta(t)$.

### Passo a Passo do Algoritmo AdaGrad

1. **Inicialização dos Parâmetros**:

- Inicialize os parâmetros do modelo $\theta$ com valores iniciais (geralmente aleatórios).
- Defina a taxa de aprendizado inicial $\eta$.
- Inicialize o acumulador de gradientes $G_t$ como uma matriz de zeros com a mesma dimensão dos parâmetros $\theta$.

2. **Cálculo do Gradiente**:

- Para cada iteração $t$, calcule o gradiente da função de perda $L$ em relação aos parâmetros $\theta(t)$: $g_t = \nabla L(\theta(t))$

3. **Acumulação dos Gradientes**:

- Atualize o acumulador de gradientes $G_t$ somando os quadrados dos gradientes:
  $G_t = G_{t-1} + g_t \odot g_t$
  Onde $\odot$ representa a operação de produto elemento a elemento (Hadamard product).

4. **Ajuste da Taxa de Aprendizado**:

- Calcule a taxa de aprendizado adaptativa para cada parâmetro utilizando o acumulador de gradientes:

  $\eta_t = \frac{\eta}{\sqrt{G_{t,ii} + \epsilon}}$
  Onde $G_{t,ii}$ é o valor acumulado para o parâmetro $i$ e $\epsilon$ é um pequeno valor para evitar divisões por zero.

5. **Atualização dos Parâmetros**:

- Atualize os parâmetros $\theta$ utilizando a taxa de aprendizado adaptativa:

  $\theta(t+1) = \theta(t) - \eta_t \odot g_t$

6. **Repetição**:

- Repita os passos acima para cada iteração até que um critério de parada seja atendido (como número máximo de iterações ou convergência da função de perda).

### Implementação em Python

```python
import numpy as np

# AdaGrad implementation
def adagrad(x, y, w, b, alpha, num_iter):
    # initialize the gradient (partial derivatives of the loss function with respect to w and b)
    grad_b = 0  # Gradient for bias

    # initialize the sum of squared gradients (used for adaptive learning rate)
    sum_w = np.zeros(w.shape)  # Sum of squared gradients for weights
    sum_b = 0  # Sum of squared gradients for bias

    # initialize the loss array to store the loss at each iteration
    loss = np.zeros(num_iter)

    # iterate for the specified number of iterations
    for i in range(num_iter):
        # calculate the gradient of the loss function with respect to w and b
        grad_w = -2 * np.dot(x.T, (y - np.dot(x, w) - b))  # Gradient for weights
        grad_b = -2 * np.sum(y - np.dot(x, w) - b)  # Gradient for bias

        # accumulate the sum of squared gradients
        sum_w += grad_w ** 2  # Update sum of squared gradients for weights
        sum_b += grad_b ** 2  # Update sum of squared gradients for bias

        # update the parameters using the Adagrad formula
        # The learning rate is scaled by the inverse square root of the accumulated squared gradients
        w = w - alpha * grad_w / (np.sqrt(sum_w) + 1e-8)  # Update weights
        b = b - alpha * grad_b / (np.sqrt(sum_b) + 1e-8)  # Update bias

        # calculate the loss (mean squared error) for the current iteration
        loss[i] = np.sum((y - np.dot(x, w) - b) ** 2) / x.shape[0]

    # return the updated weights, bias, and the loss history
    return w, b, loss
```

O AdaGrad é particularmente eficaz em problemas esparsos, onde alguns parâmetros são atualizados com mais frequência do que outros. No entanto, uma limitação do AdaGrad é que o acúmulo dos gradientes pode levar a uma redução excessiva na taxa de aprendizado ao longo do tempo, o que pode dificultar a convergência em problemas mais complexos. Apesar disso, ele introduziu conceitos importantes que serviram de base para algoritmos mais avançados, como o RMSProp e o Adam.

## RMSProp

O **RMSProp** (Root Mean Square Propagation ou Propagação da Raiz Quadrática Média) é um algoritmo de otimização que adapta a taxa de aprendizado para cada parâmetro, levando em consideração a média dos gradientes quadrados anteriores. Ele foi projetado para resolver problemas de convergência lenta em algoritmos como o AdaGrad, especialmente em problemas não convexos.

A principal ideia do RMSProp é manter uma média móvel exponencial dos gradientes quadrados, em vez de acumulá-los indefinidamente. Isso evita que a taxa de aprendizado diminua excessivamente ao longo do tempo, como ocorre no AdaGrad. Além disso, o RMSProp inclui um termo de amortecimento (damping term) para controlar a taxa de aprendizado global.

A fórmula de atualização do RMSProp é dada por:

$\theta(t+1) = \theta(t) - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \nabla L(\theta(t))$

Onde:

- $\theta(t)$ representa os parâmetros do modelo na iteração $t$.
- $\eta$ é a taxa de aprendizado inicial.
- $E[g^2]_t$ é a média móvel exponencial dos gradientes quadrados até a iteração $t$.
- $\epsilon$ é um pequeno valor adicionado para evitar divisões por zero.
- $\nabla L(\theta(t))$ é o gradiente da função de perda em relação aos parâmetros $\theta(t)$.

### Passo a Passo do Algoritmo RMSProp

1. **Inicialização dos Parâmetros**:

- Inicialize os parâmetros do modelo $\theta$ com valores iniciais.
- Defina a taxa de aprendizado inicial $\eta$.
- Inicialize o acumulador de gradientes $E[g^2]_t$ como uma matriz de zeros com a mesma dimensão dos parâmetros $\theta$.
- Defina o fator de decaimento $\rho$ (geralmente $\rho = 0.9$).

2. **Cálculo do Gradiente**:

- Para cada iteração $t$, calcule o gradiente da função de perda $L$ em relação aos parâmetros $\theta(t)$: $g_t = \nabla L(\theta(t))$.

3. **Atualização da Média Móvel Exponencial**:

- Atualize o acumulador de gradientes com a média móvel exponencial:
  $E[g^2]_t = \rho E[g^2]_{t-1} + (1 - \rho) g_t^2$.

4. **Ajuste da Taxa de Aprendizado**:

- Calcule a taxa de aprendizado adaptativa para cada parâmetro utilizando o acumulador de gradientes:
  $\eta_t = \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}$.

5. **Atualização dos Parâmetros**:

- Atualize os parâmetros $\theta$ utilizando a taxa de aprendizado adaptativa:
  $\theta(t+1) = \theta(t) - \eta_t \odot g_t$.

6. **Repetição**:

- Repita os passos acima para cada iteração até que um critério de parada seja atendido.

### Implementação em Python

```python
import numpy as np

# RMSProp implementation
def rmsprop(x, y, w, b, alpha, num_iter, rho=0.9, epsilon=1e-8):
   # initialize the gradient accumulators
   Eg_w = np.zeros(w.shape)  # Accumulator for weights
   Eg_b = 0  # Accumulator for bias

   # initialize the loss array to store the loss at each iteration
   loss = np.zeros(num_iter)

   # iterate for the specified number of iterations
   for i in range(num_iter):
      # calculate the gradient of the loss function with respect to w and b
      grad_w = -2 * np.dot(x.T, (y - np.dot(x, w) - b))  # Gradient for weights
      grad_b = -2 * np.sum(y - np.dot(x, w) - b)  # Gradient for bias

      # update the accumulators with the exponential moving average of squared gradients
      Eg_w = rho * Eg_w + (1 - rho) * grad_w ** 2
      Eg_b = rho * Eg_b + (1 - rho) * grad_b ** 2

      # update the parameters using the RMSProp formula
      w = w - alpha * grad_w / (np.sqrt(Eg_w) + epsilon)  # Update weights
      b = b - alpha * grad_b / (np.sqrt(Eg_b) + epsilon)  # Update bias

      # calculate the loss (mean squared error) for the current iteration
      loss[i] = np.sum((y - np.dot(x, w) - b) ** 2) / x.shape[0]

   # return the updated weights, bias, and the loss history
   return w, b, loss
```

O RMSProp é amplamente utilizado em problemas de aprendizado profundo devido à sua capacidade de lidar com gradientes de diferentes magnitudes e sua eficiência em problemas não convexos. Ele também é a base para o algoritmo Adam, que combina as ideias do RMSProp e do Momentum para melhorar ainda mais o desempenho da otimização.

## Adam

## Adam

O **Adam** (Adaptive Moment Estimation ou Estimativa de Momento Adaptativo) é um algoritmo de otimização que combina as ideias do **Momentum** e do **RMSProp** para melhorar a eficiência e a estabilidade do treinamento de modelos de aprendizado de máquina, especialmente redes neurais profundas. Ele é amplamente utilizado devido à sua capacidade de lidar com gradientes esparsos, adaptar a taxa de aprendizado para cada parâmetro e corrigir os momentos iniciais.

### Principais Características do Adam

1. **Momentum**: O Adam utiliza a média móvel exponencial dos gradientes passados para acelerar a convergência em direções consistentes. Isso ajuda a suavizar as atualizações e evita oscilações excessivas.

2. **RMSProp**: O algoritmo também rastreia a média móvel exponencial dos gradientes quadráticos passados, permitindo que a taxa de aprendizado seja adaptada para cada parâmetro com base na magnitude dos gradientes.

3. **Correção de Viés**: Para corrigir o viés introduzido nos momentos iniciais (quando as médias móveis ainda estão próximas de zero), o Adam inclui termos de correção de viés, garantindo que as estimativas sejam mais precisas.

### Fórmulas do Adam

A atualização dos pesos no Adam é realizada utilizando as seguintes fórmulas:

1. **Cálculo do Gradiente**:

$g_t = \nabla L(\theta(t))$

2. **Atualização do Momento (Primeiro Momento)**:
   $
  m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
  $

3. **Atualização do RMSProp (Segundo Momento)**:
   $
  v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
  $

4. **Correção de Viés**:
   $
  \hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
  $

5. **Atualização dos Pesos**:
   $
  \theta(t+1) = \theta(t) - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
  $

Onde:

- $ \theta(t) $: Parâmetros do modelo na iteração $( t )$.
- $g_t$: Gradiente da função de perda em relação aos parâmetros $ \theta(t)$.
- $ m_t $: Média móvel exponencial dos gradientes (primeiro momento).
- $ v_t $: Média móvel exponencial dos gradientes quadráticos (segundo momento).
- $ \beta_1, \beta_2 $: Taxas de decaimento para os momentos (geralmente $ \beta_1 = 0.9 $ e $ \beta_2 = 0.999 $).
- $ \eta $: Taxa de aprendizado.
- $ \epsilon $: Pequeno valor para evitar divisões por zero (geralmente $ \epsilon = 10^{-8}$).

### Implementação em Python

```python
import numpy as np

# Adam implementation
def adam(x, y, w, b, alpha, num_iter, beta1=0.9, beta2=0.999, epsilon=1e-8):
   # Initialize moment estimates
   m_w, m_b = np.zeros(w.shape), 0  # First moment (mean)
   v_w, v_b = np.zeros(w.shape), 0  # Second moment (variance)

   # Initialize loss array
   loss = np.zeros(num_iter)

   for t in range(1, num_iter + 1):
      # Compute gradients
      grad_w = -2 * np.dot(x.T, (y - np.dot(x, w) - b))
      grad_b = -2 * np.sum(y - np.dot(x, w) - b)

      # Update biased first moment estimate
      m_w = beta1 * m_w + (1 - beta1) * grad_w
      m_b = beta1 * m_b + (1 - beta1) * grad_b

      # Update biased second moment estimate
      v_w = beta2 * v_w + (1 - beta2) * (grad_w ** 2)
      v_b = beta2 * v_b + (1 - beta2) * (grad_b ** 2)

      # Correct bias in first and second moment estimates
      m_w_hat = m_w / (1 - beta1 ** t)
      m_b_hat = m_b / (1 - beta1 ** t)
      v_w_hat = v_w / (1 - beta2 ** t)
      v_b_hat = v_b / (1 - beta2 ** t)

      # Update parameters
      w = w - alpha * m_w_hat / (np.sqrt(v_w_hat) + epsilon)
      b = b - alpha * m_b_hat / (np.sqrt(v_b_hat) + epsilon)

      # Compute loss
      loss[t - 1] = np.sum((y - np.dot(x, w) - b) ** 2) / x.shape[0]

   return w, b, loss
```

O Adam é amplamente utilizado em aprendizado profundo devido à sua robustez e eficiência. Ele combina as vantagens do Momentum e do RMSProp, tornando-o adequado para uma ampla variedade de problemas de otimização. Além disso, sua capacidade de ajustar dinamicamente a taxa de aprendizado para cada parâmetro o torna uma escolha padrão em muitos frameworks de aprendizado de máquina.
