# Questão 3

Apresente um estudo comparando os seguintes algoritmos de otimização:

- Gradiente Estocástico (SGM)
- AdaGrad
- RMSProp
- Adam

Estes métodos ou otimizadores são utilizados no processo de aprendizagem de redes neurais/deep learning.

Os algoritmos de otimização desempenham um papel crucial no
treinamento de redes neurais, ajustando seus parâmetros para minimizar a função de perda e, consequentemente, melhorar o desempenho do modelo. Um exemplo clássico é o Gradient Descent (GD), que utiliza o gradiente da função de custo para atualizar os pesos da rede em direção ao mínimo global. No entanto, o GD pode enfrentar dificuldades, como ficar preso em mínimos locais. Para superar essas limitações, surgiram variações e métodos mais avançados, como o SGD, o AdaGrad, o RMSProp e o Adam, que serão discutidos em detalhes a seguir.

## Gradiente Estocástico (SGM)

O Gradiente Estocástico (SGM) é uma variação do método de Gradiente Descendente, amplamente utilizado para otimização em aprendizado de máquina. No método do Gradiente Descendente (GD), a atualização dos pesos é realizada utilizando o gradiente calculado em relação a todo o conjunto de dados.

A fórmula de atualização é dada por:

$\theta(t+1) = \theta(t) - \eta \nabla L(\theta(t))$

Onde:

- $\theta(t)$ representa os parâmetros do modelo na iteração $t$.
- $\eta$ é a taxa de aprendizado.
- $\nabla L(\theta(t))$ é o gradiente da função de perda $L$ em relação aos parâmetros $\theta(t)$, calculado para todo o conjunto de dados.

Embora o GD seja eficaz para encontrar mínimos globais em funções convexas, ele pode ser computacionalmente caro em grandes conjuntos de dados, pois requer o cálculo do gradiente para todos os exemplos a cada iteração.

A melhoria introduzida pelo **Gradiente Estocástico (SGM)** está na redução do custo computacional por iteração. Em vez de calcular o gradiente para todo o conjunto de dados, o SGM utiliza apenas um exemplo ou um mini-batch, como mostrado anteriormente. Isso permite atualizações mais frequentes dos pesos, acelerando o processo de treinamento e tornando-o mais eficiente para grandes volumes de dados. Além disso, o ruído introduzido pelo SGM pode ajudar o modelo a escapar de mínimos locais, o que é uma vantagem em problemas não convexos.

A atualização dos pesos no SGM segue a fórmula:

$\theta(t+1) = \theta(t) - \eta \nabla L(\theta(t); x_i, y_i)$

Onde:

- $\theta(t)$ representa os parâmetros do modelo na iteração $t$.
- $\eta$ é a taxa de aprendizado.
- $\nabla L(\theta(t); x_i, y_i)$ é o gradiente da função de perda $L$ em relação aos parâmetros $\theta(t)$, calculado para o exemplo $(x_i, y_i)$.
- $(x_i, y_i)$ é um par de exemplo de entrada e saída do conjunto de dados.

Essa abordagem reduz o custo computacional por iteração, tornando o treinamento mais rápido, especialmente em grandes conjuntos de dados. No entanto, o SGM introduz ruído no cálculo do gradiente, o que pode levar a oscilações durante a convergência. Apesar disso, o ruído pode ajudar o modelo a escapar de mínimos locais, tornando-o útil em problemas complexos.

O Gradiente Estocástico é simples de implementar e serve como base para muitos algoritmos de otimização mais avançados, como **AdaGrad**, **RMSProp** e **Adam**.

## AdaGrad

O **AdaGrad** (Adaptive Gradient Algorithm) é um método de otimização que busca melhorar o desempenho do Gradiente Estocástico (SGM) ao adaptar a taxa de aprendizado para cada parâmetro do modelo de forma individual. A principal proposta do AdaGrad é lidar com o problema de escalas diferentes nos gradientes dos parâmetros, ajustando automaticamente a magnitude das atualizações com base no histórico de gradientes acumulados.

A ideia central do AdaGrad é acumular os quadrados dos gradientes de cada parâmetro ao longo do tempo e usar essa informação para ajustar a taxa de aprendizado. Parâmetros que possuem gradientes maiores recebem atualizações menores, enquanto parâmetros com gradientes menores recebem atualizações maiores. Isso permite que o algoritmo se adapte dinamicamente às características do problema, promovendo uma convergência mais eficiente.

Esse processo permite que o AdaGrad ajuste dinamicamente a magnitude das atualizações para cada parâmetro, promovendo uma convergência mais eficiente em problemas com características variadas.

A fórmula de atualização do AdaGrad é dada por:

$\theta(t+1) = \theta(t) - \frac{\eta}{\sqrt{G_{t,ii} + \epsilon}} \nabla L(\theta(t))$

Onde:

- $\theta(t)$ representa os parâmetros do modelo na iteração $t$.
- $\eta$ é a taxa de aprendizado inicial.
- $G_{t,ii}$ é a soma acumulada dos quadrados dos gradientes para o parâmetro $i$ até a iteração $t$.
- $\epsilon$ é um pequeno valor adicionado para evitar divisões por zero.
- $\nabla L(\theta(t))$ é o gradiente da função de perda em relação aos parâmetros $\theta(t)$.

### Passo a Passo do Algoritmo AdaGrad

1. **Inicialização dos Parâmetros**:

- Inicialize os parâmetros do modelo $\theta$ com valores iniciais (geralmente aleatórios).
- Defina a taxa de aprendizado inicial $\eta$.
- Inicialize o acumulador de gradientes $G_t$ como uma matriz de zeros com a mesma dimensão dos parâmetros $\theta$.

2. **Cálculo do Gradiente**:

- Para cada iteração $t$, calcule o gradiente da função de perda $L$ em relação aos parâmetros $\theta(t)$: $g_t = \nabla L(\theta(t))$

3. **Acumulação dos Gradientes**:

- Atualize o acumulador de gradientes $G_t$ somando os quadrados dos gradientes:
  $G_t = G_{t-1} + g_t \odot g_t$
  Onde $\odot$ representa a operação de produto elemento a elemento (Hadamard product).

4. **Ajuste da Taxa de Aprendizado**:

- Calcule a taxa de aprendizado adaptativa para cada parâmetro utilizando o acumulador de gradientes:

  $\eta_t = \frac{\eta}{\sqrt{G_{t,ii} + \epsilon}}$
  Onde $G_{t,ii}$ é o valor acumulado para o parâmetro $i$ e $\epsilon$ é um pequeno valor para evitar divisões por zero.

5. **Atualização dos Parâmetros**:

- Atualize os parâmetros $\theta$ utilizando a taxa de aprendizado adaptativa:

  $\theta(t+1) = \theta(t) - \eta_t \odot g_t$

6. **Repetição**:

- Repita os passos acima para cada iteração até que um critério de parada seja atendido (como número máximo de iterações ou convergência da função de perda).

### Implementação em Python

```python
import numpy as np

# AdaGrad implementation
def adagrad(x, y, w, b, alpha, num_iter):
    # initialize the gradient (partial derivatives of the loss function with respect to w and b)
    grad_b = 0  # Gradient for bias

    # initialize the sum of squared gradients (used for adaptive learning rate)
    sum_w = np.zeros(w.shape)  # Sum of squared gradients for weights
    sum_b = 0  # Sum of squared gradients for bias

    # initialize the loss array to store the loss at each iteration
    loss = np.zeros(num_iter)

    # iterate for the specified number of iterations
    for i in range(num_iter):
        # calculate the gradient of the loss function with respect to w and b
        grad_w = -2 * np.dot(x.T, (y - np.dot(x, w) - b))  # Gradient for weights
        grad_b = -2 * np.sum(y - np.dot(x, w) - b)  # Gradient for bias

        # accumulate the sum of squared gradients
        sum_w += grad_w ** 2  # Update sum of squared gradients for weights
        sum_b += grad_b ** 2  # Update sum of squared gradients for bias

        # update the parameters using the Adagrad formula
        # The learning rate is scaled by the inverse square root of the accumulated squared gradients
        w = w - alpha * grad_w / (np.sqrt(sum_w) + 1e-8)  # Update weights
        b = b - alpha * grad_b / (np.sqrt(sum_b) + 1e-8)  # Update bias

        # calculate the loss (mean squared error) for the current iteration
        loss[i] = np.sum((y - np.dot(x, w) - b) ** 2) / x.shape[0]

    # return the updated weights, bias, and the loss history
    return w, b, loss
```

O AdaGrad é particularmente eficaz em problemas esparsos, onde alguns parâmetros são atualizados com mais frequência do que outros. No entanto, uma limitação do AdaGrad é que o acúmulo dos gradientes pode levar a uma redução excessiva na taxa de aprendizado ao longo do tempo, o que pode dificultar a convergência em problemas mais complexos. Apesar disso, ele introduziu conceitos importantes que serviram de base para algoritmos mais avançados, como o RMSProp e o Adam.
