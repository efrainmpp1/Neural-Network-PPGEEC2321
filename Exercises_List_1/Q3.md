# Questão 3

Apresente um estudo comparando os seguintes algoritmos de otimização:

- Gradiente Estocástico (SGM)
- AdaGrad
- RMSProp
- Adam

Estes métodos ou otimizadores são utilizados no processo de aprendizagem de redes neurais/deep learning.

Os algoritmos de otimização desempenham um papel crucial no
treinamento de redes neurais, ajustando seus parâmetros para minimizar a função de perda e, consequentemente, melhorar o desempenho do modelo. Um exemplo clássico é o Gradient Descent (GD), que utiliza o gradiente da função de custo para atualizar os pesos da rede em direção ao mínimo global. No entanto, o GD pode enfrentar dificuldades, como ficar preso em mínimos locais. Para superar essas limitações, surgiram variações e métodos mais avançados, como o SGD, o AdaGrad, o RMSProp e o Adam, que serão discutidos em detalhes a seguir.

## Gradiente Estocástico (SGM)

O Gradiente Estocástico (SGM) é uma variação do método de Gradiente Descendente, amplamente utilizado para otimização em aprendizado de máquina. No método do Gradiente Descendente (GD), a atualização dos pesos é realizada utilizando o gradiente calculado em relação a todo o conjunto de dados.

A fórmula de atualização é dada por:

$\theta(t+1) = \theta(t) - \eta \nabla L(\theta(t))$

Onde:

- $\theta(t)$ representa os parâmetros do modelo na iteração $t$.
- $\eta$ é a taxa de aprendizado.
- $\nabla L(\theta(t))$ é o gradiente da função de perda $L$ em relação aos parâmetros $\theta(t)$, calculado para todo o conjunto de dados.

Embora o GD seja eficaz para encontrar mínimos globais em funções convexas, ele pode ser computacionalmente caro em grandes conjuntos de dados, pois requer o cálculo do gradiente para todos os exemplos a cada iteração.

A melhoria introduzida pelo **Gradiente Estocástico (SGM)** está na redução do custo computacional por iteração. Em vez de calcular o gradiente para todo o conjunto de dados, o SGM utiliza apenas um exemplo ou um mini-batch, como mostrado anteriormente. Isso permite atualizações mais frequentes dos pesos, acelerando o processo de treinamento e tornando-o mais eficiente para grandes volumes de dados. Além disso, o ruído introduzido pelo SGM pode ajudar o modelo a escapar de mínimos locais, o que é uma vantagem em problemas não convexos.

A atualização dos pesos no SGM segue a fórmula:

$\theta(t+1) = \theta(t) - \eta \nabla L(\theta(t); x_i, y_i)$

Onde:

- $\theta(t)$ representa os parâmetros do modelo na iteração $t$.
- $\eta$ é a taxa de aprendizado.
- $\nabla L(\theta(t); x_i, y_i)$ é o gradiente da função de perda $L$ em relação aos parâmetros $\theta(t)$, calculado para o exemplo $(x_i, y_i)$.
- $(x_i, y_i)$ é um par de exemplo de entrada e saída do conjunto de dados.

Essa abordagem reduz o custo computacional por iteração, tornando o treinamento mais rápido, especialmente em grandes conjuntos de dados. No entanto, o SGM introduz ruído no cálculo do gradiente, o que pode levar a oscilações durante a convergência. Apesar disso, o ruído pode ajudar o modelo a escapar de mínimos locais, tornando-o útil em problemas complexos.

O Gradiente Estocástico é simples de implementar e serve como base para muitos algoritmos de otimização mais avançados, como **AdaGrad**, **RMSProp** e **Adam**.

## AdaGrad

O **AdaGrad** (Adaptive Gradient Algorithm) é um método de otimização que busca melhorar o desempenho do Gradiente Estocástico (SGM) ao adaptar a taxa de aprendizado para cada parâmetro do modelo de forma individual. A principal proposta do AdaGrad é lidar com o problema de escalas diferentes nos gradientes dos parâmetros, ajustando automaticamente a magnitude das atualizações com base no histórico de gradientes acumulados.

A ideia central do AdaGrad é acumular os quadrados dos gradientes de cada parâmetro ao longo do tempo e usar essa informação para ajustar a taxa de aprendizado. Parâmetros que possuem gradientes maiores recebem atualizações menores, enquanto parâmetros com gradientes menores recebem atualizações maiores. Isso permite que o algoritmo se adapte dinamicamente às características do problema, promovendo uma convergência mais eficiente.

Esse processo permite que o AdaGrad ajuste dinamicamente a magnitude das atualizações para cada parâmetro, promovendo uma convergência mais eficiente em problemas com características variadas.

A fórmula de atualização do AdaGrad é dada por:

$\theta(t+1) = \theta(t) - \frac{\eta}{\sqrt{G_{t,ii} + \epsilon}} \nabla L(\theta(t))$

Onde:

- $\theta(t)$ representa os parâmetros do modelo na iteração $t$.
- $\eta$ é a taxa de aprendizado inicial.
- $G_{t,ii}$ é a soma acumulada dos quadrados dos gradientes para o parâmetro $i$ até a iteração $t$.
- $\epsilon$ é um pequeno valor adicionado para evitar divisões por zero.
- $\nabla L(\theta(t))$ é o gradiente da função de perda em relação aos parâmetros $\theta(t)$.

### Passo a Passo do Algoritmo AdaGrad

1. **Inicialização dos Parâmetros**:

- Inicialize os parâmetros do modelo $\theta$ com valores iniciais (geralmente aleatórios).
- Defina a taxa de aprendizado inicial $\eta$.
- Inicialize o acumulador de gradientes $G_t$ como uma matriz de zeros com a mesma dimensão dos parâmetros $\theta$.

2. **Cálculo do Gradiente**:

- Para cada iteração $t$, calcule o gradiente da função de perda $L$ em relação aos parâmetros $\theta(t)$: $g_t = \nabla L(\theta(t))$

3. **Acumulação dos Gradientes**:

- Atualize o acumulador de gradientes $G_t$ somando os quadrados dos gradientes:
  $G_t = G_{t-1} + g_t \odot g_t$
  Onde $\odot$ representa a operação de produto elemento a elemento (Hadamard product).

4. **Ajuste da Taxa de Aprendizado**:

- Calcule a taxa de aprendizado adaptativa para cada parâmetro utilizando o acumulador de gradientes:

  $\eta_t = \frac{\eta}{\sqrt{G_{t,ii} + \epsilon}}$
  Onde $G_{t,ii}$ é o valor acumulado para o parâmetro $i$ e $\epsilon$ é um pequeno valor para evitar divisões por zero.

5. **Atualização dos Parâmetros**:

- Atualize os parâmetros $\theta$ utilizando a taxa de aprendizado adaptativa:

  $\theta(t+1) = \theta(t) - \eta_t \odot g_t$

6. **Repetição**:

- Repita os passos acima para cada iteração até que um critério de parada seja atendido (como número máximo de iterações ou convergência da função de perda).

### Implementação em Python

```python
import numpy as np

# AdaGrad implementation
def adagrad(x, y, w, b, alpha, num_iter):
    # initialize the gradient (partial derivatives of the loss function with respect to w and b)
    grad_b = 0  # Gradient for bias

    # initialize the sum of squared gradients (used for adaptive learning rate)
    sum_w = np.zeros(w.shape)  # Sum of squared gradients for weights
    sum_b = 0  # Sum of squared gradients for bias

    # initialize the loss array to store the loss at each iteration
    loss = np.zeros(num_iter)

    # iterate for the specified number of iterations
    for i in range(num_iter):
        # calculate the gradient of the loss function with respect to w and b
        grad_w = -2 * np.dot(x.T, (y - np.dot(x, w) - b))  # Gradient for weights
        grad_b = -2 * np.sum(y - np.dot(x, w) - b)  # Gradient for bias

        # accumulate the sum of squared gradients
        sum_w += grad_w ** 2  # Update sum of squared gradients for weights
        sum_b += grad_b ** 2  # Update sum of squared gradients for bias

        # update the parameters using the Adagrad formula
        # The learning rate is scaled by the inverse square root of the accumulated squared gradients
        w = w - alpha * grad_w / (np.sqrt(sum_w) + 1e-8)  # Update weights
        b = b - alpha * grad_b / (np.sqrt(sum_b) + 1e-8)  # Update bias

        # calculate the loss (mean squared error) for the current iteration
        loss[i] = np.sum((y - np.dot(x, w) - b) ** 2) / x.shape[0]

    # return the updated weights, bias, and the loss history
    return w, b, loss
```

O AdaGrad é particularmente eficaz em problemas esparsos, onde alguns parâmetros são atualizados com mais frequência do que outros. No entanto, uma limitação do AdaGrad é que o acúmulo dos gradientes pode levar a uma redução excessiva na taxa de aprendizado ao longo do tempo, o que pode dificultar a convergência em problemas mais complexos. Apesar disso, ele introduziu conceitos importantes que serviram de base para algoritmos mais avançados, como o RMSProp e o Adam.

## RMSProp

## RMSProp

O **RMSProp** (Root Mean Square Propagation ou Propagação da Raiz Quadrática Média) é um algoritmo de otimização que adapta a taxa de aprendizado para cada parâmetro, levando em consideração a média dos gradientes quadrados anteriores. Ele foi projetado para resolver problemas de convergência lenta em algoritmos como o AdaGrad, especialmente em problemas não convexos.

A principal ideia do RMSProp é manter uma média móvel exponencial dos gradientes quadrados, em vez de acumulá-los indefinidamente. Isso evita que a taxa de aprendizado diminua excessivamente ao longo do tempo, como ocorre no AdaGrad. Além disso, o RMSProp inclui um termo de amortecimento (damping term) para controlar a taxa de aprendizado global.

A fórmula de atualização do RMSProp é dada por:

$\theta(t+1) = \theta(t) - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \nabla L(\theta(t))$

Onde:

- $\theta(t)$ representa os parâmetros do modelo na iteração $t$.
- $\eta$ é a taxa de aprendizado inicial.
- $E[g^2]_t$ é a média móvel exponencial dos gradientes quadrados até a iteração $t$.
- $\epsilon$ é um pequeno valor adicionado para evitar divisões por zero.
- $\nabla L(\theta(t))$ é o gradiente da função de perda em relação aos parâmetros $\theta(t)$.

### Passo a Passo do Algoritmo RMSProp

1. **Inicialização dos Parâmetros**:

- Inicialize os parâmetros do modelo $\theta$ com valores iniciais.
- Defina a taxa de aprendizado inicial $\eta$.
- Inicialize o acumulador de gradientes $E[g^2]_t$ como uma matriz de zeros com a mesma dimensão dos parâmetros $\theta$.
- Defina o fator de decaimento $\rho$ (geralmente $\rho = 0.9$).

2. **Cálculo do Gradiente**:

- Para cada iteração $t$, calcule o gradiente da função de perda $L$ em relação aos parâmetros $\theta(t)$: $g_t = \nabla L(\theta(t))$.

3. **Atualização da Média Móvel Exponencial**:

- Atualize o acumulador de gradientes com a média móvel exponencial:
  $E[g^2]_t = \rho E[g^2]_{t-1} + (1 - \rho) g_t^2$.

4. **Ajuste da Taxa de Aprendizado**:

- Calcule a taxa de aprendizado adaptativa para cada parâmetro utilizando o acumulador de gradientes:
  $\eta_t = \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}$.

5. **Atualização dos Parâmetros**:

- Atualize os parâmetros $\theta$ utilizando a taxa de aprendizado adaptativa:
  $\theta(t+1) = \theta(t) - \eta_t \odot g_t$.

6. **Repetição**:

- Repita os passos acima para cada iteração até que um critério de parada seja atendido.

### Implementação em Python

```python
import numpy as np

# RMSProp implementation
def rmsprop(x, y, w, b, alpha, num_iter, rho=0.9, epsilon=1e-8):
   # initialize the gradient accumulators
   Eg_w = np.zeros(w.shape)  # Accumulator for weights
   Eg_b = 0  # Accumulator for bias

   # initialize the loss array to store the loss at each iteration
   loss = np.zeros(num_iter)

   # iterate for the specified number of iterations
   for i in range(num_iter):
      # calculate the gradient of the loss function with respect to w and b
      grad_w = -2 * np.dot(x.T, (y - np.dot(x, w) - b))  # Gradient for weights
      grad_b = -2 * np.sum(y - np.dot(x, w) - b)  # Gradient for bias

      # update the accumulators with the exponential moving average of squared gradients
      Eg_w = rho * Eg_w + (1 - rho) * grad_w ** 2
      Eg_b = rho * Eg_b + (1 - rho) * grad_b ** 2

      # update the parameters using the RMSProp formula
      w = w - alpha * grad_w / (np.sqrt(Eg_w) + epsilon)  # Update weights
      b = b - alpha * grad_b / (np.sqrt(Eg_b) + epsilon)  # Update bias

      # calculate the loss (mean squared error) for the current iteration
      loss[i] = np.sum((y - np.dot(x, w) - b) ** 2) / x.shape[0]

   # return the updated weights, bias, and the loss history
   return w, b, loss
```

O RMSProp é amplamente utilizado em problemas de aprendizado profundo devido à sua capacidade de lidar com gradientes de diferentes magnitudes e sua eficiência em problemas não convexos. Ele também é a base para o algoritmo Adam, que combina as ideias do RMSProp e do Momentum para melhorar ainda mais o desempenho da otimização.
