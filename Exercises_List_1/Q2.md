# Questão 2

Considere a função $E(\mathbf{w})$ onde $\mathbf{w} = [w_1, w_2, \dots, w_n]^t$ é um vetor com múltiplas variáveis. Usando a expansão em série de Taylor, a função pode ser expressa como:

$$
E(\mathbf{w}(n) + \Delta \mathbf{w}(n)) = E(\mathbf{w}(n)) + \mathbf{g}^t(\mathbf{w}(n)) \Delta \mathbf{w}(n) + \frac{1}{2} \Delta \mathbf{w}^t(n) \mathbf{H}(\mathbf{w}(n)) \Delta \mathbf{w}(n) + O(\|\Delta \mathbf{w}\|^3),
$$

onde $\mathbf{g}(\mathbf{w}(n))$ é o vetor gradiente local definido por $\mathbf{g}(\mathbf{w}) = \frac{\partial E(\mathbf{w})}{\partial \mathbf{w}}$ e $\mathbf{H}(\mathbf{w})$ é a matriz Hessiana, definida por $\mathbf{H}(\mathbf{w}) = \frac{\partial^2 E(\mathbf{w})}{\partial \mathbf{w}^2}$.

Demonstre com base na expansão em série de Taylor:

### a) Que o método do gradiente da descida mais íngreme é dado por: $\mathbf{w}(n+1) = \mathbf{w}(n) - \eta \mathbf{g}(\mathbf{w}(n))$ . Onde $\eta$ é o coeficiente de aprendizagem ou passo do método.

Para demonstrar que o método do gradiente da descida mais íngreme é dado por $\mathbf{w}(n+1) = \mathbf{w}(n) - \eta \mathbf{g}(\mathbf{w}(n))$, considerando a expansão em série de Taylor fornecida.

O objetivo do método do gradiente da descida mais íngreme é minimizar a função de erro $E(\mathbf{w})$. Para isso, escolhemos $\Delta \mathbf{w}(n)$ de forma que a direção de atualização seja oposta ao gradiente $\mathbf{g}(\mathbf{w}(n))$, pois o gradiente aponta para a direção de maior crescimento da função.

Desconsiderando os termos de ordem superior ($O(\|\Delta \mathbf{w}\|^3)$) e assumindo que $\Delta \mathbf{w}(n)$ é pequeno, podemos aproximar a variação da função de erro como:

$$
E(\mathbf{w}(n) + \Delta \mathbf{w}(n)) \approx E(\mathbf{w}(n)) + \mathbf{g}^t(\mathbf{w}(n)) \Delta \mathbf{w}(n).
$$

Para minimizar $E(\mathbf{w}(n) + \Delta \mathbf{w}(n))$, escolhemos $\Delta \mathbf{w}(n)$ na direção oposta ao gradiente, ou seja:

$$
\Delta \mathbf{w}(n) = -\eta \mathbf{g}(\mathbf{w}(n)),
$$

onde $\eta > 0$ é o coeficiente de aprendizagem que controla o tamanho do passo.

Substituindo $\Delta \mathbf{w}(n)$ na equação de atualização do vetor $\mathbf{w}$, temos:

$$
\mathbf{w}(n+1) = \mathbf{w}(n) + \Delta \mathbf{w}(n).
$$

Logo:

$$
\mathbf{w}(n+1) = \mathbf{w}(n) - \eta \mathbf{g}(\mathbf{w}(n)).
$$

Portanto, demonstramos que o método do gradiente da descida mais íngreme é dado por:

$$
\mathbf{w}(n+1) = \mathbf{w}(n) - \eta \mathbf{g}(\mathbf{w}(n)).
$$
