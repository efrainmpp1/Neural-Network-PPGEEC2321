# Questão 2

Considere a função $E(\mathbf{w})$ onde $\mathbf{w} = [w_1, w_2, \dots, w_n]^t$ é um vetor com múltiplas variáveis. Usando a expansão em série de Taylor, a função pode ser expressa como:

$$
E(\mathbf{w}(n) + \Delta \mathbf{w}(n)) = E(\mathbf{w}(n)) + \mathbf{g}^t(\mathbf{w}(n)) \Delta \mathbf{w}(n) + \frac{1}{2} \Delta \mathbf{w}^t(n) \mathbf{H}(\mathbf{w}(n)) \Delta \mathbf{w}(n) + O(\|\Delta \mathbf{w}\|^3),
$$

onde $\mathbf{g}(\mathbf{w}(n))$ é o vetor gradiente local definido por $\mathbf{g}(\mathbf{w}) = \frac{\partial E(\mathbf{w})}{\partial \mathbf{w}}$ e $\mathbf{H}(\mathbf{w})$ é a matriz Hessiana, definida por $\mathbf{H}(\mathbf{w}) = \frac{\partial^2 E(\mathbf{w})}{\partial \mathbf{w}^2}$.

**Demonstre com base na expansão em série de Taylor:**

### a) Que o método do gradiente da descida mais íngreme é dado por: $\mathbf{w}(n+1) = \mathbf{w}(n) - \eta \mathbf{g}(\mathbf{w}(n))$ . Onde $\eta$ é o coeficiente de aprendizagem ou passo do método.

Para demonstrar que o método do gradiente da descida mais íngreme é dado por $\mathbf{w}(n+1) = \mathbf{w}(n) - \eta \mathbf{g}(\mathbf{w}(n))$, considerando a expansão em série de Taylor fornecida.

Substituindo $\Delta \mathbf{w}(n) = -\eta \mathbf{g}(\mathbf{w}(n))$ na expansão em série de Taylor, temos:

$$
E(\mathbf{w}(n+1)) = E(\mathbf{w}(n) - \eta \mathbf{g}(\mathbf{w}(n)))
$$

Expandindo a expressão, obtemos:

$$
E(\mathbf{w}(n+1)) \approx E(\mathbf{w}(n)) + \mathbf{g}^t(\mathbf{w}(n))(-\eta \mathbf{g}(\mathbf{w}(n))) + \frac{1}{2}(-\eta \mathbf{g}(\mathbf{w}(n)))^t \mathbf{H}(\mathbf{w}(n))(-\eta \mathbf{g}(\mathbf{w}(n))) + O(\eta^3)
$$

Simplificando os termos:

$$
E(\mathbf{w}(n+1)) \approx E(\mathbf{w}(n)) - \eta \mathbf{g}^t(\mathbf{w}(n)) \mathbf{g}(\mathbf{w}(n)) + \frac{1}{2} \eta^2 \mathbf{g}^t(\mathbf{w}(n)) \mathbf{H}(\mathbf{w}(n)) \mathbf{g}(\mathbf{w}(n)) + O(\eta^3)
$$

Quando utilizamos o método do gradiente descendente, o objetivo é minimizar a função de erro $E(\mathbf{w})$, ou seja, queremos encontrar o mínimo da função. Para isso, buscamos a direção de maior descida da função, que é oposta à direção do gradiente $\mathbf{g}(\mathbf{w}(n))$. A regra de atualização do método do gradiente descendente é simplesmente atualizar os pesos $\mathbf{w}(n)$ na direção do gradiente, com um fator de aprendizado $\eta$ que determina o tamanho do passo de atualização.

Assim, a partir da aproximação acima, podemos aproximar $E(\mathbf{w}(n+1))$ por $E(\mathbf{w}(n)) - \eta \mathbf{g}^t(\mathbf{w}(n)) \mathbf{g}(\mathbf{w}(n))$. Para isso, basta igualar essa aproximação a $E(\mathbf{w}(n+1))$ e manipular a expressão para chegar em uma expressão para $\mathbf{w}(n+1)$. Isso pode ser feito da seguinte forma:

$$
E(\mathbf{w}(n+1)) = E(\mathbf{w}(n)) - \eta \mathbf{g}^t(\mathbf{w}(n)) \mathbf{g}(\mathbf{w}(n))
$$

Como queremos encontrar a atualização para os pesos $\mathbf{w}$, basta pegar a derivada em relação a $\mathbf{w}(n+1)$ em ambos os lados:

$$
\frac{\partial E(\mathbf{w}(n+1))}{\partial \mathbf{w}(n+1)} \approx \frac{\partial E(\mathbf{w}(n))}{\partial \mathbf{w}(n+1)} - \frac{\eta \mathbf{g}^t(\mathbf{w}(n)) \mathbf{g}(\mathbf{w}(n))}{\partial \mathbf{w}(n+1)}
$$

Note que a derivada da expressão à direita é zero, uma vez que o gradiente local $\mathbf{g}(\mathbf{w}(n))$ não depende de $\mathbf{w}(n+1)$. Logo, temos:

$$
\frac{\partial E(\mathbf{w}(n+1))}{\partial \mathbf{w}(n+1)} \approx \frac{\partial E(\mathbf{w}(n))}{\partial \mathbf{w}(n+1)}
$$

Agora, podemos substituir $\mathbf{w}(n+1)$ por $\mathbf{w}(n) - \eta \mathbf{g}(\mathbf{w}(n))$ para obter:

$$
\frac{\partial E(\mathbf{w}(n+1))}{\partial \mathbf{w}(n+1)} \approx \frac{\partial E(\mathbf{w}(n))}{\partial \mathbf{w}(n+1)} = \frac{\partial E(\mathbf{w}(n))}{\partial \mathbf{w}(n)} \cdot \frac{\partial \mathbf{w}(n)}{\partial \mathbf{w}(n+1)}
$$

$$
= -\mathbf{g}^t(\mathbf{w}(n))
$$

Assim, chegamos em:

$$
-\mathbf{g}^t(\mathbf{w}(n+1)) \approx -\mathbf{g}^t(\mathbf{w}(n))
$$

Ou seja:

$$
\mathbf{g}^t(\mathbf{w}(n+1)) \approx \mathbf{g}^t(\mathbf{w}(n))
$$

E, finalmente, temos a regra de atualização do método do gradiente descendente:

$$
\mathbf{w}(n+1) = \mathbf{w}(n) - \eta \mathbf{g}(\mathbf{w}(n))
$$

Essa regra de atualização nos permite atualizar os pesos da rede neural em direção ao mínimo da função de erro $E(\mathbf{w})$.

### b) Que o método de Newton é dado por $\mathbf{w}(n+1) = \mathbf{w}(n) + \mathbf{H}^{-1}(\mathbf{w}(n)) \mathbf{g}(\mathbf{w}(n))$

Podemos demonstrar sua origem a partir da expansão em série de Taylor. Considerando a aproximação de segunda ordem para a função $E(\mathbf{w})$:

$$
E(\mathbf{w}(n) + \Delta \mathbf{w}(n)) \approx E(\mathbf{w}(n)) + \mathbf{g}^t(\mathbf{w}(n)) \Delta \mathbf{w}(n) + \frac{1}{2} \Delta \mathbf{w}^t(n) \mathbf{H}(\mathbf{w}(n)) \Delta \mathbf{w}(n),
$$

o objetivo é minimizar $E(\mathbf{w})$. Para isso, derivamos a expressão acima em relação a $\Delta \mathbf{w}(n)$ e igualamos a zero:

$$
\frac{\partial E(\mathbf{w}(n) + \Delta \mathbf{w}(n))}{\partial \Delta \mathbf{w}(n)} = \mathbf{g}(\mathbf{w}(n)) + \mathbf{H}(\mathbf{w}(n)) \Delta \mathbf{w}(n) = 0.
$$

Resolvendo para $\Delta \mathbf{w}(n)$, obtemos:

$$
\Delta \mathbf{w}(n) = -\mathbf{H}^{-1}(\mathbf{w}(n)) \mathbf{g}(\mathbf{w}(n)).
$$

Substituímos $\Delta \mathbf{w}(n)$ na regra de atualização $\mathbf{w}(n+1) = \mathbf{w}(n) + \Delta \mathbf{w}(n)$, resultando em:

$$
\mathbf{w}(n+1) = \mathbf{w}(n) - \mathbf{H}^{-1}(\mathbf{w}(n)) \mathbf{g}(\mathbf{w}(n)).
$$

Portanto, o método de Newton é dado por:

$$
\mathbf{w}(n+1) = \mathbf{w}(n) + \mathbf{H}^{-1}(\mathbf{w}(n)) \mathbf{g}(\mathbf{w}(n)).
$$
